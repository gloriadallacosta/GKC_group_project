{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tripadvisor_pieces_p1 (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gEoWRLEUMli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6548023a-e08c-497c-eece-f3233117f456"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[K     |████████████████████████████████| 904 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [561 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,320 kB]\n",
            "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,428 kB]\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [717 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,796 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,755 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [919 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,202 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [594 kB]\n",
            "Fetched 13.6 MB in 4s (3,759 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 91.8 MB of archives.\n",
            "After this operation, 315 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 92.0.4515.159-0ubuntu0.18.04.1 [1,124 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 92.0.4515.159-0ubuntu0.18.04.1 [81.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 92.0.4515.159-0ubuntu0.18.04.1 [4,026 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 92.0.4515.159-0ubuntu0.18.04.1 [4,902 kB]\n",
            "Fetched 91.8 MB in 5s (20.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 148492 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_92.0.4515.159-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_92.0.4515.159-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (92.0.4515.159-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSTdWLSZUER-"
      },
      "source": [
        "import requests\n",
        "from lxml import html, etree\n",
        "from selenium.webdriver import Chrome, ChromeOptions\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import csv\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6eh50cXUJub"
      },
      "source": [
        "def data_writer(li):\n",
        "    with open(r'output.csv', mode=\"a\",\n",
        "              newline=\"\", encoding=\"utf-8-sig\") as f_out:\n",
        "        my_writer = csv.writer(f_out, delimiter=',')\n",
        "        my_writer.writerow(li)\n",
        "data_writer(['URL', \"Title\", \"Location\",\n",
        "            \"No. of Reviews\", \"Rating\", \"Price Range\", \"Cuisine\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COluc3O8UVnq"
      },
      "source": [
        "BASE = 'https://www.tripadvisor.com'\n",
        "\n",
        "##Chrome Driver Options##\n",
        "width = range(500, 700, 10)\n",
        "height = range(800, 1000, 20)\n",
        "chrome_options = ChromeOptions()\n",
        "chrome_options.add_argument(\n",
        "    f\"window-size={random.choice(width)},{random.choice(height)}\")\n",
        "chrome_options.add_argument('--no-default-browser-check')\n",
        "chrome_options.add_argument('--disable-gpu')\n",
        "chrome_options.add_argument('--disable-extensions')\n",
        "chrome_options.add_argument('--disable-default-apps')\n",
        "chrome_options.add_argument('--disable-notifications')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.add_argument('--log-level=3')\n",
        "chrome_options.add_argument('--headless')\n",
        "wd = Chrome('chromedriver',options=chrome_options)\n",
        "wd.get('https://www.tripadvisor.com/Restaurants-g187849-Milan_Lombardy.html')\n",
        "try:\n",
        "    wd.find_element_by_xpath('//button[@id=\"_evidon-accept-button\"]').click()\n",
        "except:\n",
        "    pass\n",
        "time.sleep(2)\n",
        "wd.find_element_by_xpath('//span[contains(text(), \"Takeout\")]').click()\n",
        "time.sleep(3)\n",
        "tree = html.fromstring(wd.page_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyGr5yMwUd16"
      },
      "source": [
        "for item in tree.xpath('//div[@data-component-props=\"page-manifest\"]/div//span/text()'):\n",
        "    try:\n",
        "        num_results = int(item)\n",
        "        break\n",
        "    except:\n",
        "        pass\n",
        "a_li = [0]\n",
        "a = 0\n",
        "for i in range(int(num_results/30)):\n",
        "    a += 30\n",
        "    a_li.append(a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zyc9jdxVJmg",
        "outputId": "b7231788-5286-4de5-bdf8-215b3775c078"
      },
      "source": [
        "headers = {\n",
        "    'authority': 'www.tripadvisor.com',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"',\n",
        "    'x-requested-with': 'XMLHttpRequest',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36',\n",
        "    'accept': '*/*',\n",
        "    'sec-fetch-site': 'same-origin',\n",
        "    'sec-fetch-mode': 'cors',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'referer': 'https://www.tripadvisor.com/Restaurants-g187849-Milan_Lombardy.html',\n",
        "    'accept-language': 'en-US,en;q=0.9,ar;q=0.8',\n",
        "    \"cookie\":''\n",
        "}\n",
        "\n",
        "page_counter = 1\n",
        "restaurant_urls_li = []\n",
        "for a in a_li:\n",
        "    params = (\n",
        "    ('Action', 'PAGE'),\n",
        "    ('ajax', '1'),\n",
        "    ('availSearchEnabled', 'true'),\n",
        "    ('sortOrder', 'relevance'),\n",
        "    ('geo', '187849'),\n",
        "    ('itags', '10591'),\n",
        "    ('zfp', '10601'),\n",
        "    ('eaterydate', datetime.today().strftime('%Y_%m_%d')),\n",
        "    ('date', (datetime.today()+timedelta(days=1)).strftime('%Y-%m-%d')),\n",
        "    ('time', '20:00:00'),\n",
        "    ('people', '2'),\n",
        "    ('o', f'a{a}'),\n",
        "    )\n",
        "    r = requests.get(f'https://www.tripadvisor.com/RestaurantSearch', params=params, headers=headers)\n",
        "    tree = html.fromstring(r.content)\n",
        "    list_of_restaurants = tree.xpath('//div[contains(@data-test,\"list_item\")]')\n",
        "    for restaurant in list_of_restaurants:\n",
        "        restaurant_url = f\"{BASE}{restaurant.xpath('.//a')[0].attrib['href']}\"\n",
        "        restaurant_urls_li.append(restaurant_url)\n",
        "    print(f'Page No. {page_counter} scrapped')\n",
        "    page_counter += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page No. 1 scrapped\n",
            "Page No. 2 scrapped\n",
            "Page No. 3 scrapped\n",
            "Page No. 4 scrapped\n",
            "Page No. 5 scrapped\n",
            "Page No. 6 scrapped\n",
            "Page No. 7 scrapped\n",
            "Page No. 8 scrapped\n",
            "Page No. 9 scrapped\n",
            "Page No. 10 scrapped\n",
            "Page No. 11 scrapped\n",
            "Page No. 12 scrapped\n",
            "Page No. 13 scrapped\n",
            "Page No. 14 scrapped\n",
            "Page No. 15 scrapped\n",
            "Page No. 16 scrapped\n",
            "Page No. 17 scrapped\n",
            "Page No. 18 scrapped\n",
            "Page No. 19 scrapped\n",
            "Page No. 20 scrapped\n",
            "Page No. 21 scrapped\n",
            "Page No. 22 scrapped\n",
            "Page No. 23 scrapped\n",
            "Page No. 24 scrapped\n",
            "Page No. 25 scrapped\n",
            "Page No. 26 scrapped\n",
            "Page No. 27 scrapped\n",
            "Page No. 28 scrapped\n",
            "Page No. 29 scrapped\n",
            "Page No. 30 scrapped\n",
            "Page No. 31 scrapped\n",
            "Page No. 32 scrapped\n",
            "Page No. 33 scrapped\n",
            "Page No. 34 scrapped\n",
            "Page No. 35 scrapped\n",
            "Page No. 36 scrapped\n",
            "Page No. 37 scrapped\n",
            "Page No. 38 scrapped\n",
            "Page No. 39 scrapped\n",
            "Page No. 40 scrapped\n",
            "Page No. 41 scrapped\n",
            "Page No. 42 scrapped\n",
            "Page No. 43 scrapped\n",
            "Page No. 44 scrapped\n",
            "Page No. 45 scrapped\n",
            "Page No. 46 scrapped\n",
            "Page No. 47 scrapped\n",
            "Page No. 48 scrapped\n",
            "Page No. 49 scrapped\n",
            "Page No. 50 scrapped\n",
            "Page No. 51 scrapped\n",
            "Page No. 52 scrapped\n",
            "Page No. 53 scrapped\n",
            "Page No. 54 scrapped\n",
            "Page No. 55 scrapped\n",
            "Page No. 56 scrapped\n",
            "Page No. 57 scrapped\n",
            "Page No. 58 scrapped\n",
            "Page No. 59 scrapped\n",
            "Page No. 60 scrapped\n",
            "Page No. 61 scrapped\n",
            "Page No. 62 scrapped\n",
            "Page No. 63 scrapped\n",
            "Page No. 64 scrapped\n",
            "Page No. 65 scrapped\n",
            "Page No. 66 scrapped\n",
            "Page No. 67 scrapped\n",
            "Page No. 68 scrapped\n",
            "Page No. 69 scrapped\n",
            "Page No. 70 scrapped\n",
            "Page No. 71 scrapped\n",
            "Page No. 72 scrapped\n",
            "Page No. 73 scrapped\n",
            "Page No. 74 scrapped\n",
            "Page No. 75 scrapped\n",
            "Page No. 76 scrapped\n",
            "Page No. 77 scrapped\n",
            "Page No. 78 scrapped\n",
            "Page No. 79 scrapped\n",
            "Page No. 80 scrapped\n",
            "Page No. 81 scrapped\n",
            "Page No. 82 scrapped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyWS5VZD9OKL",
        "outputId": "2460f768-a773-4dd0-a4e4-e4bac743282a"
      },
      "source": [
        "print(f'You have {len(restaurant_urls_li)} restaurants.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 3021 restaurants.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic9I8vQ1FJgs"
      },
      "source": [
        "list_saved = DataFrame(restaurant_urls_li)\n",
        "list_saved.to_csv(r\"list_saved.csv\")\n",
        "restaurant_urls_li = pd.read_csv(r\"list_saved.csv\")\n",
        "restaurant_urls_li = restaurant_urls_li.iloc[:,-1:]\n",
        "restaurant_urls_li = restaurant_urls_li.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12Nnj5qgBtF8",
        "outputId": "3e66784e-bc00-4f92-e25b-a4e5317b5b06"
      },
      "source": [
        "num_rest = len(restaurant_urls_li)\n",
        "rest_x_piece = 300\n",
        "value = random.randint(0,10)\n",
        "print (value)\n",
        "if value == 0:\n",
        "  start = rest_x_piece * 0\n",
        "  end = rest_x_piece * 1\n",
        "elif value == 1:\n",
        "  start = rest_x_piece * 1\n",
        "  end = rest_x_piece * 2\n",
        "elif value == 2:\n",
        "  start = rest_x_piece * 2\n",
        "  end = rest_x_piece * 3\n",
        "elif value == 3:\n",
        "  start = rest_x_piece * 3\n",
        "  end = rest_x_piece * 4\n",
        "elif value == 4:\n",
        "  start = rest_x_piece * 4\n",
        "  end = rest_x_piece * 5\n",
        "elif value == 5:\n",
        "  start = rest_x_piece * 5\n",
        "  end = rest_x_piece * 6\n",
        "elif value == 6:\n",
        "  start = rest_x_piece * 6\n",
        "  end = rest_x_piece * 7\n",
        "elif value == 7:\n",
        "  start = rest_x_piece * 7\n",
        "  end = rest_x_piece * 8\n",
        "elif value == 8:\n",
        "  start = rest_x_piece * 8\n",
        "  end = rest_x_piece * 9\n",
        "elif value == 9:\n",
        "  start = rest_x_piece * 9\n",
        "  end = rest_x_piece * 10\n",
        "elif value == 10:\n",
        "  start = rest_x_piece * 10\n",
        "  end = num_rest\n",
        "start = int(start)\n",
        "end = int(end)\n",
        "print (start)  \n",
        "print (end)         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "1800\n",
            "2100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNWA91oXuUod"
      },
      "source": [
        "for restaurant_url in restaurant_urls_li[start:end]:\n",
        "    wd.get(restaurant_url[0])\n",
        "    tree = html.fromstring(wd.page_source)\n",
        "    restaurant = tree.xpath('//div[contains(@id,\"taplc_top_info\")]')[0]\n",
        "    try:\n",
        "        restaurant_name = restaurant.xpath('.//h1')[0].text\n",
        "        for item in restaurant.xpath('.//a'):\n",
        "            try:\n",
        "                item_href = item.attrib['href']\n",
        "                if item_href == '#MAPVIEW':\n",
        "                    restaurant_location = item.text\n",
        "                if item_href == '#REVIEWS':\n",
        "                    restaurant_reviews_num = item.xpath('.//span')[0].text\n",
        "                    restaurant_reviews_rating = item.xpath(\n",
        "                        './/svg')[0].attrib['title'].split(' ')[0]\n",
        "            except:\n",
        "                pass\n",
        "        try:\n",
        "            price = tree.xpath(\n",
        "                '//div[contains(text(),\"PRICE RANGE\")]/following-sibling::div')[0].text\n",
        "        except:\n",
        "            price = ''\n",
        "        try:\n",
        "            cuisine = tree.xpath(\n",
        "            '//div[contains(text(),\"CUISINES\")]/following-sibling::div')[0].text\n",
        "        except:\n",
        "            cuisine = ''\n",
        "        data_writer([restaurant_url, restaurant_name, restaurant_location,\n",
        "                    restaurant_reviews_num, restaurant_reviews_rating, price, cuisine])\n",
        "        print(f'{restaurant_name} Scrapped.')\n",
        "    except:\n",
        "        print(restaurant_url)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}